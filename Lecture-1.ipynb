{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the lecture on modeling qualitative data with MTA (MQD)\n",
    "\n",
    "In this lecture, we are learning how to modeling qualitative data based on texts. The lecture covers the most important steps, beginning with the installation of the software needed for the lecture and ending with the publication of the results in several forms. \n",
    "\n",
    "The lecture's structure develops along the following key steps:\n",
    "\n",
    "  1. Lecture 1: theoretical background: what is the modeling of qualitative data and which software why will be used in the lecture\n",
    "  2. Lecture 2: install the software (Anaconda; Linux on a usb-key -- install and postinstall --, python3)\n",
    "  3. Lecture 3 and 4: data driven preprocessing -- gathering text data and basic conversion; parsing text data in several format (pdf, doc·x/odt/rtf, x·htm·l, txt etc.) using low level programming utilities\n",
    "  4. Lecture 5: using MTA to model your data -- the topic modelling way, basic requirements and first analysis\n",
    "  5. Lecture 6: automation of MTA, in-depth interpretation of the results generated by MTA (in-room lecture)\n",
    "  6. Lecture 7: MTA plots, csv to generate your own plots the static way, usage of jupyter as collaborative platform for this purpose (in-room lecture)\n",
    "  7. Lecture 8: Communicate on your results -- simple widgets or how to give your results some interactivity\n",
    "  8. Lecture 9: Dashboards and Graphs -- web-app, topic networks and semantic chains (standalone example)\n",
    "  \n",
    "The lecture has been organized as a mixed of several materials, combining jupyter notebooks with code snippets that you can run within the notebook, video material covering the usage of some of the software used in this lecture, as well as some exercises that you could complete to improve your own skills. \n",
    "\n",
    "Do you have questions or inputs, or do you want to know more about topic modeling with MTA and our work in this area? You are welcome to contact me at: christian dot papilloud at soziologie dot uni-halle dot de. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling Qualitative Data -- Overview\n",
    "\n",
    " 1. Goal: learning a general workflow that you can easily implement to modeling qualitative data\n",
    " 2. Tools: \n",
    "     - Anaconda or mainstream Linux operating system \n",
    "     - dataset consisting in various text based data\n",
    "     - low level native Unix programs to preprocess your data\n",
    "     - python programs to do your analysis\n",
    "     - python programs to layout your results\n",
    "     - python programs to make presentations of your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are qualitative data and how to modeling them\n",
    "\n",
    "1. Qualitative data: mostly text based data of various types (not uniquely, f.ex. also pictures)\n",
    "    - postcards, vignettes, short texts (f.ex. tweets)\n",
    "    - newspapers' articles\n",
    "    - scientific articles\n",
    "    - books\n",
    "    - corpora = collections of text data, i.e. big (qualitative) data\n",
    "\n",
    "2. Modeling techniques: techniques to understand the information content of your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Several modeling techniques -- Exclusively human-centric techniques\n",
    "\n",
    "1. human-centric techniques:\n",
    "   - based on reading the dataset and interpret it\n",
    "   - categorization of the information content in the dataset\n",
    "   - structuration of the categories based on the interpretation of dataset\n",
    "   - result: analysis of this structure = output the hidden/latent structure of the dataset\n",
    "        \n",
    "     * advantages: \n",
    "       - based on the human understanding of texts = better control over the interpretation of texts\n",
    "       - sensible to polysemic meaning of words/texts\n",
    "     * disadvantages:\n",
    "       - difficult to scale results to the total amount of the investigated material -- main results often apply to 10-15% of the investigated material       \n",
    "       - difficult to generalized results out of the given dataset\n",
    "       - difficult to reproduce the results\n",
    "       - difficult to share the results with other researchers\n",
    "       - difficult to generalized the results to other sources/actors where the dataset comes from\n",
    "       - possible interpretation bias\n",
    "       - time consuming --> often limit the scope of data that can be investigated in a given time\n",
    "       - complete quantitative oriented research designs, but not compatible with them = parallel routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Several modeling techniques -- Mostly human-centric techniques\n",
    "\n",
    "2. mostly human-centric techniques, with the help of basic computing:\n",
    "   - based on reading the dataset and interpret it\n",
    "   - categorization of the information content in the dataset -- computer driven\n",
    "   - basic statistics (mostly frequencies of words' occurrences and distribution of words)\n",
    "   - structuration of the categories based on the interpretation of dataset -- computer driven\n",
    "   - basic structuration tools (f.ex. MAXQDA, NLP techniques)\n",
    "   - result: analysis of this structure = output the hidden/latent structure of the dataset\n",
    "   \n",
    "     * advantages: \n",
    "       - based on the human understanding of texts, add computer driven facilities\n",
    "       - sensible to polysemic meaning of words/texts\n",
    "       - better at generalizing the results out of the given dataset than exclusively human-centric techniques\n",
    "       - less time consuming than human-centric techniques\n",
    "     * disadvantages:\n",
    "       - difficult to scale results to the total amount of the investigated material -- main results often apply to 10-15% of the investigated material       \n",
    "       - difficult to reproduce the results\n",
    "       - difficult to share the results with other researchers\n",
    "       - difficult to generalized the results to other sources/actors where the dataset comes from\n",
    "       - possible interpretation bias\n",
    "       - complete quantitative oriented research designs, but not compatible with them = parallel routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Several modeling techniques -- Partly human-centric techniques\n",
    "\n",
    "3. partly human-centric techniques, partly computer driven:\n",
    "   - reading the dataset and processing it is computer driven\n",
    "   - categorization of the information content in the dataset -- computer driven\n",
    "   - advanced analytics using statistic or mathematic modelling methods\n",
    "   - structuration of the categories based on the modelling methods\n",
    "   - advanced structuration tools (f.ex. R, Python)\n",
    "   - result: analysis of this structure = output the hidden/latent structure of the dataset --> rests on human understanding of the results\n",
    "   \n",
    "     * advantages:\n",
    "       - based on the human understanding of texts, add advanced data analytic\n",
    "       - better scaling of the results --> apply to the total amount of the investigated material\n",
    "       - better at generalizing the results out of the given dataset than other human-centric techniques\n",
    "       - less time consuming than other human-centric techniques\n",
    "       - results can easily be reproduced\n",
    "       - results can easily be shared with other researchers\n",
    "       - better at generalizing the results to other sources/actors where the dataset comes from\n",
    "       - better at accumulating further data to enrich the dataset\n",
    "       - better at comparing same kind of data in different languages\n",
    "       - reduce the interpretation bias\n",
    "       - better compatibility with quantitative oriented research designs = converging routes\n",
    "     * disadvantages:\n",
    "       - less sensible to polysemic meaning of words/texts (even in AI frameworks)\n",
    "       - knowledge demanding --> skills in programming (which can be time consuming)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Right tools for the right tasks -- Operating systems\n",
    "\n",
    "1. Why Linux?\n",
    "  - opensource operating system -- easy to install and maintain at no economic costs\n",
    "  - take the most out of dated hardware --> reuse your old computers\n",
    "  - portable -- use the OS on a lot of hardware, as well as from simple external drives or USB keys\n",
    "  - mainstream software for all mainstream tasks\n",
    "  - powerful software for data analytic:\n",
    "    - install R CRAN and related packages \n",
    "    - Python comes native with the operating system\n",
    "    - benefit from native unix low programming utilities to tailor the dataset\n",
    "    - deliver opensource free software to extend the analytic framework\n",
    "    \n",
    "2. Why not Windows or MacOS (or * BSD)?\n",
    "    - cost of the operating system and the software\n",
    "    - no portability of the software to other hardware -- you have to stick with one given hardware\n",
    "    - Windows: no out-of-the-box tools to tailor the dataset --> limited choice of unix tools compatible with Windows\n",
    "    - MacOS and * BSD flavors: some out-of-the-box tools to tailor the dataset --> not always compatible with same unix tools -- * BSD OS are more involving\n",
    "    - but: you can install R (directly) and Python (with f.ex. Anaconda) for data analytic\n",
    "    - but: you can install opensource free software to extend the analytic framework\n",
    "    \n",
    "Linux is more often used in the context of data analytic because of its practicability and scaling capacity. Disadvantage: coming from Windows or MacOS, there is a learning curve regarding: \n",
    "  - the use of the command line\n",
    "  - the use of equivalent software to the ones you have on Windows/MacOS --> f.ex. LibreOffice instead of Microsoft Office\n",
    "  - in general: changing some of your habits\n",
    "\n",
    "Benefit: gain in autonomy with your research purpose --> you can do and design your work and workflow as you want, i.e. you are not limited by the OS. However, if you want to work with your own non-unix operating system, we provide some advice in these lectures to do so at a minimum involving cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Right tools for the right tasks -- Software\n",
    "\n",
    "1. Why Python and not R?\n",
    "  - both are excellent software and programming environment with a long history and a great community\n",
    "  - both have a learning curve\n",
    "  - R -- mainly used for statistics\n",
    "  - Python -- more general approach to data science\n",
    "  - R -- you use the flexibility of R libraries\n",
    "  - Python -- you can write your application from scratch\n",
    "  - R -- runs locally\n",
    "  - Python -- better integration with apps and better deployment \n",
    "\n",
    "Python is often the first and evident choice when it comes to machine learning framework design -- easy to find support and material for your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About these lectures\n",
    "\n",
    "These lectures are provided in the form of a notebook that you can run and update with your own notes on your computer. \n",
    "\n",
    "In order to follow this lecture and to be able to run the code, we recommend the use of jupyter lab. You can install jupyter lab easily with your python distribution and run it privately in a browser window. Using Anaconda, you can install jupyter lab from the Anaconda package manager, or in a (base root) terminal by tipping: \n",
    "\n",
    "```\n",
    "pip install jupyterlab\n",
    "```\n",
    "\n",
    "On Linux, open a terminal and enter: \n",
    "\n",
    "```\n",
    "pip3 install jupyterlab\n",
    "```\n",
    "\n",
    "Some of the code snippets provided in this notebook are commented, i.e. they have been prefixed with the '#' sign which tells jupyter not to run such a line. You can uncomment those lines, i.e. you can remove this '#' sign in order to see what the code is doing in practice. Don't remove the exclamation mark before the code snippets when you see one of it, because jupyter needs it to run your code.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
