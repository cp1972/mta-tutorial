** Aufbau der Vorlesung zur Modellierung qualitativer Daten mit MTA (MQD)

In dieser Vorlesung lernen wir, wie man qualitative Daten auf der Grundlage von Texten modelliert. Die Vorlesung behandelt die wichtigsten Schritte, beginnend mit der Installation der für die Vorlesung benötigten Software
und endet mit der Veröffentlichung der Ergebnisse in verschiedenen Formen.

Die Struktur der Vorlesung entwickelt sich entlang der folgenden Schlüsselschritte:

1. Vorlesung 1: Theoretischer Hintergrund: Was ist die Modellierung von qualitativen Daten und welche Software
   warum in der Vorlesung verwendet wird
2. Vorlesung 2: Installation der Software (Anaconda; Linux auf einem USB-Schlüssel -- Installation und Nachinstallation --, python3)
3. Vorlesung 3 und 4: datengesteuerte Vorverarbeitung -- Erfassen von Textdaten und grundlegende Konvertierung; Parsen von Textdaten in verschiedenen Formaten (pdf, doc-x/odt/rtf, x-htm-l, txt etc.) unter Verwendung von Low-Level-Programmierung
   Dienstprogramme
4. Vorlesung 5: Verwendung von MTA zur Modellierung Ihrer Daten - das Thema Modellierung, grundlegende Anforderungen und erste Analyse
5. Vorlesung 6: Automatisierung von MTA, vertiefte Interpretation der von MTA generierten Ergebnisse (Präsenzveranstaltung)
6. Vorlesung 7: MTA-Plots, csv zur Erstellung eigener Plots auf statische Weise, Nutzung von jupyter als kollaborative Plattform für diesen Zweck (Präsenzveranstaltung)
7. Vorlesung 8: Kommunizieren Sie Ihre Ergebnisse - einfache Widgets oder wie Sie Ihre Ergebnisse interaktiv gestalten können
8. Vortrag 9: Dashboards und Graphen -- Web-App, Themennetzwerke und semantische Ketten (eigenständiges Beispiel)

Die Vorlesung ist als eine Mischung aus verschiedenen Materialien organisiert, die Jupyter-Notebooks mit Code-Schnipseln, die Sie innerhalb des Notebooks ausführen können, Videomaterial, das die Verwendung einiger der
Software, die in dieser Vorlesung verwendet wird, sowie einige Übungen, die Sie absolvieren können, um Ihre eigenen Fähigkeiten zu verbessern.

Haben Sie Fragen oder Anregungen, oder möchten Sie mehr über die Themenmodellierung mit MTA und unsere Arbeit in diesem Bereich erfahren? Sie können mich gerne kontaktieren unter: christian dot papilloud at soziologie dot
uni-halle dot de.

** Qualitative Daten modellieren - Überblick

1. Ziel: Erlernen eines allgemeinen Arbeitsablaufs, den man leicht implementieren kann, um
   Modellierung qualitativer Daten
2. Werkzeuge:

   - Anaconda oder ein gängiges Linux-Betriebssystem
   - Datensatz bestehend aus verschiedenen textbasierten Daten
   - Native Unix-Programme auf niedriger Ebene zur Vorverarbeitung Ihrer Daten
   - Python-Programme zur Durchführung Ihrer Analyse
   - Python-Programme zur Darstellung Ihrer Ergebnisse
   - Python-Programme für die Präsentation Ihrer Ergebnisse

** Was sind qualitative Daten und wie modelliert man sie?

1. Qualitative Daten: meist textbasierte Daten verschiedener Art (nicht eindeutig, z.B. auch Bilder)

   - Postkarten, Vignetten, kurze Texte (z.B. Tweets)
   - Zeitungsartikel
   - wissenschaftliche Artikel
   - Bücher
   - Korpora = Sammlungen von Textdaten, d. h. große (qualitative) Daten

2. Modellierungstechniken: Techniken zum Verständnis des Informationsgehalts
   Ihres Datensatzes

** Verschiedene Modellierungstechniken -- Ausschließlich menschenzentrierte Techniken
   PROPERTIES:
   :CUSTOM_ID: mehrere-modellierungs-techniken-ausschließlich-menschzentrierte-techniken
   :END:

1. menschenzentrierte Techniken:

   - basierend auf dem Lesen des Datensatzes und dessen Interpretation

   - Kategorisierung des Informationsgehalts des Datensatzes

   - Strukturierung der Kategorien auf der Grundlage der Interpretation des
     Datensatzes

   - Ergebnis: Analyse dieser Struktur = Ausgabe der verborgenen/ latenten Struktur des Datensatzes

     - Vorteile:

       - basierend auf dem menschlichen Verständnis von Texten = bessere Kontrolle über
         die Interpretation von Texten
       - sensibel für polysemische Bedeutung von Wörtern/Texten

     - Nachteile:

       - Schwierige Skalierung der Ergebnisse auf die Gesamtmenge des
         untersuchten Materials zu skalieren - Hauptergebnisse gelten oft für 10-15% des
         des untersuchten Materials\\
       - schwer zu verallgemeinernde Ergebnisse aus dem gegebenen Datensatz
       - schwierig, die Ergebnisse zu reproduzieren
       - schwierig, die Ergebnisse mit anderen Forschern zu teilen
       - schwierig, die Ergebnisse auf andere Quellen/Akteure zu verallgemeinern, von denen der Datensatz stammt
       - mögliche Verzerrungen bei der Interpretation
       - zeitaufwendig --> schränkt oft den Umfang der Daten ein, die in einer bestimmten Zeit untersucht werden können
       - vervollständigen quantitativ orientierte Forschungsdesigns, sind aber nicht mit diesen kompatibel = parallele Wege

** Verschiedene Modellierungstechniken - meist menschenzentrierte Techniken

2. überwiegend menschenzentrierte Techniken, mit Hilfe von Basisberechnungen:

   - basierend auf dem Lesen des Datensatzes und dessen Interpretation

   - Kategorisierung des Informationsgehalts des Datensatzes --
     computergesteuert

   - Basisstatistiken (hauptsächlich Häufigkeiten des Vorkommens von Wörtern und
     Verteilung von Wörtern)

   - Strukturierung der Kategorien auf der Grundlage der Interpretation des
     Datensatzes - computergesteuert

   - grundlegende Strukturierungswerkzeuge (z.B. MAXQDA, NLP-Techniken)

   - Ergebnis: Analyse dieser Struktur = Ausgabe der verborgenen/ latenten Struktur des Datensatzes

     - Vorteile:

       - basierend auf dem menschlichen Verständnis von Texten, zusätzlich computergesteuert
         Möglichkeiten
       - sensibel für polysemische Bedeutung von Wörtern/Texten
       - bessere Verallgemeinerbarkeit der Ergebnisse aus dem gegebenen Datensatz als ausschließlich menschenzentrierte Techniken
       - weniger zeitaufwendig als human-zentrierte Techniken

     - Nachteile:

       - Schwierige Skalierung der Ergebnisse auf die Gesamtmenge des
         untersuchten Materials zu skalieren - die Hauptergebnisse beziehen sich oft auf 10-15% des
         des untersuchten Materials\\
       - Schwierige Reproduzierbarkeit der Ergebnisse
       - schwierig, die Ergebnisse mit anderen Forschern zu teilen
       - schwierig, die Ergebnisse auf andere Quellen/Akteure zu verallgemeinern, von denen der Datensatz stammt
       - mögliche Verzerrungen bei der Interpretation
       - vollständig quantitativ orientierte Forschungsdesigns, aber nicht mit ihnen kompatibel = Parallelrouten

** Verschiedene Modellierungstechniken -- Teilweise menschenzentrierte Techniken
   PROPERTIES:
   :CUSTOM_ID: mehrere-modellierungs-techniken-teilweise-menschzentrierte-techniken
   :END:

3. Teilweise menschenzentrierte Techniken, teilweise computergesteuert:

   - Lesen des Datensatzes und dessen Verarbeitung ist computergesteuert

   - Kategorisierung des Informationsgehalts im Datensatz --
     computergesteuert

   - fortgeschrittene Analytik unter Verwendung statistischer oder mathematischer Modellierungsmethoden

   - Strukturierung der Kategorien auf der Grundlage der Modellierungsmethoden

   - fortgeschrittene Strukturierungswerkzeuge (z.B. R, Python)

   - Ergebnis: Analyse dieser Struktur = Ausgabe der verborgenen/latenten Struktur des Datensatzes --> beruht auf dem menschlichen Verständnis der Ergebnisse


    - Vorteile:

       - basierend auf dem menschlichen Verständnis von Texten, erweiterte Daten
         Analyse
       - bessere Skalierung der Ergebnisse --> Anwendung auf die Gesamtmenge des untersuchten Materials
       - bessere Verallgemeinerbarkeit der Ergebnisse aus dem gegebenen Datensatz als bei anderen menschenzentrierten Techniken
       - weniger zeitaufwändig als andere humanzentrierte Verfahren
       - die Ergebnisse können leicht reproduziert werden
       - die Ergebnisse können leicht mit anderen Forschern geteilt werden
       - bessere Verallgemeinerbarkeit der Ergebnisse auf andere Quellen/Akteure, von denen der Datensatz stammt
       - besser bei der Sammlung weiterer Daten zur Anreicherung des Datensatzes
       - besserer Vergleich der gleichen Art von Daten in verschiedenen Sprachen
       - Verringerung der Interpretationsverzerrung
       - bessere Kompatibilität mit quantitativ orientierten Forschungsdesigns = konvergierende Wege

     - Nachteile:

       - weniger sensibel für polysemische Bedeutung von Wörtern/Texten (auch in AI
         Frameworks)
       - Wissen erforderlich --> Programmierkenntnisse (die zeitaufwendig sein können)
       
** Die richtigen Werkzeuge für die richtigen Aufgaben -- Software

Warum Python und nicht R? - beides sind exzellente Software und Programmierumgebungen mit einer langen Geschichte und einer großen Community - beides hat eine Lernkurve - R -- hauptsächlich für Statistik verwendet - Python --
allgemeinerer Ansatz für Data Science - R -- man nutzt die Flexibilität der R-Bibliotheken - Python -- man kann seine Anwendung von Grund auf neu schreiben - R -- läuft lokal - Python -- bessere Integration mit Anwendungen
und bessere Bereitstellung

Python ist oft die erste und offensichtliche Wahl, wenn es um die Entwicklung von Frameworks für maschinelles Lernen geht - man findet leicht Unterstützung und Material für seine Arbeit

** Warum Topic Modeling und nicht die übliche Netzwerkanalyse

Themenmodellierungstechniken sind konzeptionell nahe an Netzwerkanalysetechniken, in dem Sinne, dass sie darauf abzielen, Netzwerke von Beziehungen zwischen Daten zu beschreiben. Der große Unterschied ist, dass die Netzwerk
Analyse jedes Beziehungsereignis als Gegenstand der Analyse betrachtet. Themenmodellierungstechniken versuchen, eine solche Komplexität auf eine Reihe von Clustern zu reduzieren, die möglicherweise die zugrunde liegenden
Strukturen der Daten erklären.  In diesem Sinne versucht die Themenanalyse, eine Grundlage für die Interpretation von Beziehungsereignissen in einem allgemeineren Rahmen zu liefern, der erklären kann, was die Daten zusammenhält.

In gewissem Sinne liegt die Themenmodellierung zwischen einem statistischen Ansatz für Daten, der versucht, Dimensionen auf einige wichtige zu reduzieren, und der Netzwerkanalyse, die versucht, das kohärente Ausmaß von
relationaler Ereignisse zu messen. Sie stellt eine Kompromisslösung zwischen diesen beiden Ansätzen dar, da sie relationale Ereignisse auf wichtige Dimensionen reduzieren kann, die sie strukturieren, während sie gleichzeitig die Möglichkeit bietet
Möglichkeit bietet, diese Dimensionen auch zu vergrößern (um den Preis weniger scharf differenzierter Themen oder weniger kohärenter Themen).

Die Themenmodellierung ist sowohl als Faktor im Rahmen einer quantitativen Studie als auch als Instrument zur Neugestaltung von Netzwerkanalyseergebnissen sinnvoll. Vom Standpunkt der sozialwissenschaftlichen Theorien aus gesehen, passt die Themen
Modellierung am besten in den theoretischen Rahmen von relationalen Ansätzen zu gesellschaftlichen Fragen, bei denen die analytische Einheit die Beziehung zwischen Daten ist, aus der man die Bedeutung dieser Daten ableitet.
dieser Daten ableitet.

Die Themenmodellierung ist offen für künftige Entwicklungen, die mehr KI-Tools integrieren (wie z. B. BERT-Tools als Lösung für semantische Einbettungen auf Wort- oder Wort-zu-Dokument-Ebene), auch wenn dies manchmal auf Kosten der
der Ressourcen des Computers. Mit MTA haben wir einen Einstiegspunkt in solche Frameworks, da MTA word2vec als KI-Tool auf niedriger Ebene zur Modellierung von Wort- und Wort-zu-Dokument-Einbettungen verwendet. Aber zur Zeit
MTA keine fortschrittlicheren KI-Tools, um kosteneffizient zu bleiben, und weil solche Tools noch experimentell sind und weiter entwickelt werden müssen, bevor sie in einen Workflow wie
in einen Arbeitsablauf wie den von MTA vorgeschlagenen integriert werden können.

Bei der Themenanalyse handelt es sich um eine explorative Methode, die es ermöglicht, robuste Einblicke in die Daten zu erhalten, die eine möglicherweise bessere Interpretation der Daten unter dem Gesichtspunkt ihrer
Beziehungen. Die Ergebnisse, die Sie mit der Themenanalyse erhalten, hängen daher von der Art und Weise ab, wie Sie Ihre Daten aufbereiten und wie Sie Ihre Analyse durchführen. In diesen Vorlesungen schlagen wir einen Arbeitsablauf vor, der
einen Arbeitsablauf vor, der es Ihnen ermöglichen sollte, die besten Ergebnisse aus Ihren Daten herauszuholen - wir bieten eine Art überwachten Arbeitsablauf für die Analyse von unüberwachten Daten.

** Über diese Vorlesungen

Diese Vorlesungen werden in Form eines Notizbuchs zur Verfügung gestellt, das Sie auf Ihrem Computer ausführen und mit Ihren eigenen Notizen aktualisieren können.

Um dieser Vorlesung zu folgen und den Code ausführen zu können, empfehlen wir die Verwendung von jupyter lab. Sie können jupyter lab einfach mit Ihrer Python-Distribution installieren und es privat in einem Browser
fenster ausführen.  Wenn Sie Anaconda verwenden, können Sie jupyter lab über den Anaconda-Paketmanager oder in einem (Basis-Root-)Terminal durch Tippen installieren:

#+BEGIN_EXAMPLE
  pip install jupyterlab
#+END_EXAMPLE

Unter Linux öffnen Sie ein Terminal und geben Sie ein:

#+BEGIN_EXAMPLE
  pip3 install jupyterlab
#+END_EXAMPLE

Einige der Codeschnipsel in diesem Notizbuch sind auskommentiert, d.h. sie sind mit einem vorangestellten '#'-Zeichen versehen, das jupyter anweist, eine solche Zeile nicht auszuführen. Sie können diese Zeilen unkommentiert lassen, d.h. Sie
können dieses '#'-Zeichen entfernen, um zu sehen, was der Code in der Praxis tut. Entfernen Sie nicht das Ausrufezeichen vor den Codeschnipseln, wenn Sie eines sehen, denn jupyter braucht es, um Ihren
Code auszuführen.
